{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universidad del Valle de Guatemala\n",
    "## Facultad de Ingenier√≠a\n",
    "### Departamento de Computaci√≥n\n",
    "\n",
    "---\n",
    "\n",
    "# Laboratorio 9: Visualizaciones Interactivas\n",
    "\n",
    "**Integrantes:**\n",
    "- Diego Alexander Hern√°ndez Silvestre, 21270\n",
    "- Linda In√©s Jim√©nez Vides, 21169\n",
    "\n",
    "**Curso:** Data Science  \n",
    "**Secci√≥n:** 10  \n",
    "\n",
    "---\n",
    "\n",
    "Guatemala, 20 de octubre de 2024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk, spacy, textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import opendatasets as od\n",
    "import streamlit as st\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interactive\n",
    "from IPython.display import display, clear_output\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from matplotlib_venn import venn2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from matplotlib.colors import ListedColormap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printText(df, stop = 10):\n",
    "    for i, t in enumerate(df):\n",
    "        print(i, t)\n",
    "        if i >= stop:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('nlp-getting-started'):\n",
    "    od.download(\"https://www.kaggle.com/c/nlp-getting-started/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTrain = pd.read_csv('nlp-getting-started/train.csv')\n",
    "dataTest = pd.read_csv('nlp-getting-started/test.csv')\n",
    "dataSampleSubmission = pd.read_csv('nlp-getting-started/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = dataTrain['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "textWithoutUrl = text.str.replace(r'http\\S+|www\\S+', '', regex=True)\n",
    "#printText(textWithoutUrl, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "textLowerCase = textWithoutUrl.str.lower()\n",
    "#textLowerCase.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "textWithouthSpecialCharacthers = textLowerCase.str.replace('@', '')\n",
    "textWithouthSpecialCharacthers = textWithouthSpecialCharacthers.str.replace('#', '')\n",
    "textWithouthSpecialCharacthers = textWithouthSpecialCharacthers.str.replace(\"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "textWithouthSpecialCharacthers = textWithouthSpecialCharacthers.str.replace(r'\\?+', '?', regex=True)\n",
    "textWithouthSpecialCharacthers = textWithouthSpecialCharacthers.str.replace(r'\\!+', '!', regex=True)\n",
    "textWithouthSpecialCharacthers = textWithouthSpecialCharacthers.str.replace('&amp;', 'and')\n",
    "textWithouthSpecialCharacthers = textWithouthSpecialCharacthers.str.replace('\\n', ' ')\n",
    "#printText(textWithouthSpecialCharacthers, stop=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "textWithouthSpecialCharacthers = textWithouthSpecialCharacthers.str.replace(r'(\\.{3,})', 'THREEPOINTSIDENFIFIER', regex=True)\n",
    "textWithouthSpecialCharacthers = textWithouthSpecialCharacthers.str.replace(r'(.)\\1{'+str(2)+',}', r'\\1' * 2, regex=True)\n",
    "textWithouthSpecialCharacthers = textWithouthSpecialCharacthers.str.replace('THREEPOINTSIDENFIFIER', '...')\n",
    "#textWithouthSpecialCharacthers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "textWithouthSpecialCharacthers = textWithouthSpecialCharacthers.str.replace('.',' ')\n",
    "textWithouthSpecialCharacthers = textWithouthSpecialCharacthers.str.replace(',',' ')\n",
    "#printText(textWithouthSpecialCharacthers, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "textWithouthSpecialCharacthers = textWithouthSpecialCharacthers.str.replace(r'\\s{2,}', ' ', regex=True)\n",
    "#printText(textWithouthSpecialCharacthers, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\daher\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "def removeStopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stopWords]  \n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "textWithoutStopwords = textWithouthSpecialCharacthers.apply(removeStopwords)\n",
    "#printText(textWithoutStopwords, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "textWithoutStopwords = textWithoutStopwords.str.replace(r'[^\\w\\s]','',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\daher\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "tokenizedText = textWithoutStopwords.apply(word_tokenize)\n",
    "allWords = [word for words in tokenizedText for word in words]\n",
    "wordCounts = Counter(allWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostFrequentWords(tokenizedTextColumn, targetColumn):\n",
    "    #print(\"üîÑ Analizando palabras m√°s frecuentes por categor√≠a...\")\n",
    "    \n",
    "    disasterTweets = tokenizedTextColumn[targetColumn == 1]\n",
    "    nonDisasterTweets = tokenizedTextColumn[targetColumn == 0]\n",
    "    \n",
    "    def countWords(texts):\n",
    "        wordsList = [word for tokens in texts for word in tokens]\n",
    "        wordCounts = Counter(wordsList)\n",
    "        return wordCounts\n",
    "    \n",
    "    disasterWordCounts = countWords(disasterTweets)\n",
    "    nonDisasterWordCounts = countWords(nonDisasterTweets)\n",
    "    disasterWordFreqDf = pd.DataFrame(disasterWordCounts.items(), columns=['Word', 'Frequency']).sort_values(by='Frequency', ascending=False)\n",
    "    nonDisasterWordFreqDf = pd.DataFrame(nonDisasterWordCounts.items(), columns=['Word', 'Frequency']).sort_values(by='Frequency', ascending=False)\n",
    "    \n",
    "    '''print(\"üìà Palabra m√°s frecuente en desastres:\")\n",
    "    print(disasterWordFreqDf.head(1))\n",
    "    \n",
    "    print(\"üìà Palabra m√°s frecuente en no desastres:\")\n",
    "    print(nonDisasterWordFreqDf.head(1))'''\n",
    "    \n",
    "    return disasterWordFreqDf, nonDisasterWordFreqDf\n",
    "\n",
    "disasterWordFreqDf, nonDisasterWordFreqDf = mostFrequentWords(tokenizedText, dataTrain['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-18 18:54:26.952 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-18 18:54:26.953 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-18 18:54:26.953 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-18 18:54:26.953 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-18 18:54:26.954 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-18 18:54:26.954 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-18 18:54:26.955 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-18 18:54:26.955 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-18 18:54:26.955 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-18 18:54:26.956 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-18 18:54:27.490 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-18 18:54:27.919 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-18 18:54:27.919 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-18 18:54:27.919 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-18 18:54:27.921 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "C:\\Users\\daher\\AppData\\Local\\Temp\\ipykernel_15648\\3789350458.py:35: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x='Frequency', y='Word', data=topWords, palette='colorblind', ax=ax)\n",
      "C:\\Users\\daher\\AppData\\Local\\Temp\\ipykernel_15648\\3789350458.py:39: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  ax.legend().remove()\n",
      "2024-10-18 18:54:27.977 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-18 18:54:28.151 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-18 18:54:28.153 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# Funci√≥n para generar nube de palabras\n",
    "def plotWordCloud(option):\n",
    "    if option == 'Desastres':\n",
    "        title = 'Desastres'\n",
    "        text = ' '.join(disasterWordFreqDf['Word'])\n",
    "    else:\n",
    "        title = 'No Desastres'\n",
    "        text = ' '.join(nonDisasterWordFreqDf['Word'])\n",
    "    \n",
    "    # Usar una paleta apta para dalt√≥nicos con seaborn\n",
    "    palette = sns.color_palette(\"colorblind\", as_cmap=False)\n",
    "    cmap = ListedColormap(palette.as_hex())\n",
    "    \n",
    "    # Crear la nube de palabras con la paleta de colores y un fondo blanco\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white', colormap=cmap).generate(text)\n",
    "    \n",
    "    # Mostrar la nube de palabras con Streamlit\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.imshow(wordcloud, interpolation='bilinear')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title)\n",
    "    st.pyplot(fig)\n",
    "\n",
    "# Funci√≥n para generar histograma de palabras m√°s frecuentes\n",
    "def plotTopWordsHistogram(option):\n",
    "    if option == 'Desastres':\n",
    "        title = 'Desastres'\n",
    "        topWords = disasterWordFreqDf.head(10)\n",
    "    else:\n",
    "        title = 'No Desastres'\n",
    "        topWords = nonDisasterWordFreqDf.head(10)\n",
    "    \n",
    "    # Crear el histograma\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    sns.barplot(x='Frequency', y='Word', data=topWords, palette='colorblind', ax=ax)\n",
    "    ax.set_xlabel('Frecuencia')\n",
    "    ax.set_ylabel('Palabras')\n",
    "    ax.set_title(f'Top 10 Palabras M√°s Frecuentes - {title}')\n",
    "    ax.legend().remove()\n",
    "    st.pyplot(fig)\n",
    "\n",
    "# Interfaz en Streamlit\n",
    "st.title('Visualizaci√≥n de Nubes de Palabras y Histogramas')\n",
    "\n",
    "# Seleccionar entre \"Desastres\" y \"No Desastres\"\n",
    "option = st.selectbox('Seleccionar categor√≠a:', ['Desastres', 'No Desastres'])\n",
    "\n",
    "# Generar nube de palabras\n",
    "st.header('Nube de Palabras')\n",
    "plotWordCloud(option)\n",
    "\n",
    "# Generar histograma de palabras m√°s frecuentes\n",
    "st.header('Histograma de Palabras M√°s Frecuentes')\n",
    "plotTopWordsHistogram(option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"interactive_plot2 = widgets.interactive(plotTopWordsHistogram, option=dropdown)\\noutput2 = interactive_plot2.children[-1]\\noutput2.layout.height = '600px'\\ninteractive_plot2\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''interactive_plot2 = widgets.interactive(plotTopWordsHistogram, option=dropdown)\n",
    "output2 = interactive_plot2.children[-1]\n",
    "output2.layout.height = '600px'\n",
    "interactive_plot2'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìä Histogramas con las palabras m√°s repetidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nplotTopWordsHistogram(disasterWordFreqDf, 'Desastres')\\nplotTopWordsHistogram(nonDisasterWordFreqDf, 'No Desastres')\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "plotTopWordsHistogram(disasterWordFreqDf, 'Desastres')\n",
    "plotTopWordsHistogram(nonDisasterWordFreqDf, 'No Desastres')'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from matplotlib_venn import venn2\\nimport seaborn as sns\\nfrom matplotlib.colors import ListedColormap\\n\\ndef commonWords(disasterWordFreqDf, nonDisasterWordFreqDf):\\n    print(\"üîç Identificando palabras comunes en ambas categor√≠as...\")\\n    disasterWords = set(disasterWordFreqDf[\\'Word\\'])\\n    nonDisasterWords = set(nonDisasterWordFreqDf[\\'Word\\'])\\n    commonWords = disasterWords.intersection(nonDisasterWords)\\n    print(\"üìö Palabras comunes en ambas categor√≠as:\")\\n    print(commonWords)\\n    \\n    # Usar una paleta colorblind\\n    palette = sns.color_palette(\"colorblind\", 2)\\n    \\n    plt.figure(figsize=(10, 7))\\n    \\n    # Definir los colores del diagrama de Venn\\n    venn = venn2([disasterWords, nonDisasterWords], (\\'Desastres\\', \\'No Desastres\\'))\\n    venn.get_label_by_id(\\'10\\').set_color(palette[0])\\n    venn.get_label_by_id(\\'01\\').set_color(palette[1])\\n    \\n    plt.title(\\'Palabras Comunes entre Desastres y No Desastres\\')\\n    plt.show()\\n\\ncommonWords(disasterWordFreqDf, nonDisasterWordFreqDf)'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from matplotlib_venn import venn2\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def commonWords(disasterWordFreqDf, nonDisasterWordFreqDf):\n",
    "    print(\"üîç Identificando palabras comunes en ambas categor√≠as...\")\n",
    "    disasterWords = set(disasterWordFreqDf['Word'])\n",
    "    nonDisasterWords = set(nonDisasterWordFreqDf['Word'])\n",
    "    commonWords = disasterWords.intersection(nonDisasterWords)\n",
    "    print(\"üìö Palabras comunes en ambas categor√≠as:\")\n",
    "    print(commonWords)\n",
    "    \n",
    "    # Usar una paleta colorblind\n",
    "    palette = sns.color_palette(\"colorblind\", 2)\n",
    "    \n",
    "    plt.figure(figsize=(10, 7))\n",
    "    \n",
    "    # Definir los colores del diagrama de Venn\n",
    "    venn = venn2([disasterWords, nonDisasterWords], ('Desastres', 'No Desastres'))\n",
    "    venn.get_label_by_id('10').set_color(palette[0])\n",
    "    venn.get_label_by_id('01').set_color(palette[1])\n",
    "    \n",
    "    plt.title('Palabras Comunes entre Desastres y No Desastres')\n",
    "    plt.show()\n",
    "\n",
    "commonWords(disasterWordFreqDf, nonDisasterWordFreqDf)'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Inciso 6. Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üóÇÔ∏è Dividir en entrenamiento y prueba (80 - 20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"sentences = textWithoutStopwords.tolist()\\ntargets = dataTrain['target'].tolist()\\ntestSentences= dataTest['text'].tolist()\\n\\ntrainingSize = int(len(sentences) * 0.80)\\ntrainingSentences = sentences[0:trainingSize]\\ntestingSentences = sentences[trainingSize:]\\ntrainingTargets = targets[0:trainingSize]\\ntestingTargets = targets[trainingSize:]\\n\\ntrainingTargetsArray = np.array(trainingTargets)\\ntestingTargetsArray = np.array(testingTargets)\\n\\nprint(trainingTargetsArray)\\nprint(testingTargetsArray)\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''sentences = textWithoutStopwords.tolist()\n",
    "targets = dataTrain['target'].tolist()\n",
    "testSentences= dataTest['text'].tolist()\n",
    "\n",
    "trainingSize = int(len(sentences) * 0.80)\n",
    "trainingSentences = sentences[0:trainingSize]\n",
    "testingSentences = sentences[trainingSize:]\n",
    "trainingTargets = targets[0:trainingSize]\n",
    "testingTargets = targets[trainingSize:]\n",
    "\n",
    "trainingTargetsArray = np.array(trainingTargets)\n",
    "testingTargetsArray = np.array(testingTargets)\n",
    "\n",
    "print(trainingTargetsArray)\n",
    "print(testingTargetsArray)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üñºÔ∏è Tokenizaci√≥n y aplicaci√≥n de padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"tokenizer = Tokenizer(num_words = 500, oov_token='<OOV>')\\ntokenizer.fit_on_texts(trainingSentences)\\nword_index = tokenizer.word_index\\ntraining_sequences = tokenizer.texts_to_sequences(trainingSentences)\\ntraining_padded = pad_sequences(training_sequences, maxlen=40, padding='post', truncating='post')\\ntesting_sequences = tokenizer.texts_to_sequences(testingSentences)\\ntesting_padded = pad_sequences(testing_sequences, maxlen=40, padding='post', truncating='post')\\nmain_test_sequence=tokenizer.texts_to_sequences(testSentences)\\nmain_test_padded=pad_sequences(main_test_sequence,maxlen=40,padding='post',truncating='post')\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''tokenizer = Tokenizer(num_words = 500, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(trainingSentences)\n",
    "word_index = tokenizer.word_index\n",
    "training_sequences = tokenizer.texts_to_sequences(trainingSentences)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=40, padding='post', truncating='post')\n",
    "testing_sequences = tokenizer.texts_to_sequences(testingSentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=40, padding='post', truncating='post')\n",
    "main_test_sequence=tokenizer.texts_to_sequences(testSentences)\n",
    "main_test_padded=pad_sequences(main_test_sequence,maxlen=40,padding='post',truncating='post')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def add_embedding_noise(embedding, noise_factor=0.1):\\n    noise = tf.random.normal(shape=tf.shape(embedding), mean=0.0, stddev=noise_factor, dtype=tf.float32)\\n    return embedding + noise'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def add_embedding_noise(embedding, noise_factor=0.1):\n",
    "    noise = tf.random.normal(shape=tf.shape(embedding), mean=0.0, stddev=noise_factor, dtype=tf.float32)\n",
    "    return embedding + noise'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üß† LSTM y Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"firstModel = tf.keras.Sequential([\\n    layers.Embedding(500, 16, input_length=40),\\n    layers.Lambda(lambda x: add_embedding_noise(x, noise_factor=0.05)),\\n    layers.Bidirectional(layers.LSTM(8, return_sequences=True)),\\n    layers.Bidirectional(layers.LSTM(8)),\\n    layers.Dense(72, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\\n    layers.Dropout(0.5),\\n    layers.BatchNormalization(),\\n    layers.Dense(36, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\\n    layers.Dropout(0.5),\\n    layers.BatchNormalization(),\\n    layers.Dense(1, activation='sigmoid')\\n])\\n\\n\\nlearning_rate = 0.001  \\noptimizer = Adam(learning_rate=learning_rate)\\n\\nfirstModel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\\nfirstModel.summary()\\nfirstModel.save('models/firstModel.h5')\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''firstModel = tf.keras.Sequential([\n",
    "    layers.Embedding(500, 16, input_length=40),\n",
    "    layers.Lambda(lambda x: add_embedding_noise(x, noise_factor=0.05)),\n",
    "    layers.Bidirectional(layers.LSTM(8, return_sequences=True)),\n",
    "    layers.Bidirectional(layers.LSTM(8)),\n",
    "    layers.Dense(72, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(36, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "learning_rate = 0.001  \n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "firstModel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "firstModel.summary()\n",
    "firstModel.save('models/firstModel.h5')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#earlyStopping=EarlyStopping(min_delta=0.001,patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'history=firstModel.fit(\\n    training_padded,\\n    trainingTargetsArray,\\n    epochs=15,\\n    validation_data=(testing_padded,testingTargetsArray),\\n    callbacks=[earlyStopping]\\n)'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''history=firstModel.fit(\n",
    "    training_padded,\n",
    "    trainingTargetsArray,\n",
    "    epochs=15,\n",
    "    validation_data=(testing_padded,testingTargetsArray),\n",
    "    callbacks=[earlyStopping]\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loss, accuracy = firstModel.evaluate(testing_padded, testingTargetsArray)\\nprint(f\"Test Loss: {loss:.4f}\")\\nprint(f\"Test Accuracy: {accuracy:.4f}\")'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''loss, accuracy = firstModel.evaluate(testing_padded, testingTargetsArray)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"plt.figure(figsize=(12, 6))\\nplt.subplot(1, 2, 1) # Loss\\nplt.plot(history.history['loss'], label='Training Loss')\\nplt.plot(history.history['val_loss'], label='Validation Loss')\\nplt.title('Training and Validation Loss')\\nplt.xlabel('Epochs')\\nplt.ylabel('Loss')\\nplt.legend()\\nplt.subplot(1, 2, 2) # Accuracy\\nplt.plot(history.history['accuracy'], label='Training Accuracy')\\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\\nplt.title('Training and Validation Accuracy')\\nplt.xlabel('Epochs')\\nplt.ylabel('Accuracy')\\nplt.legend()\\nplt.show()\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1) # Loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2) # Accuracy\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üß† CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"secondModel = tf.keras.Sequential([\\n    layers.Embedding(500, 16, input_length=40),\\n    layers.Conv1D(128, 5, activation='relu'),\\n    layers.MaxPooling1D(pool_size=4),\\n    layers.Conv1D(64, 5, activation='relu'),\\n    layers.MaxPooling1D(pool_size=4),\\n    layers.Flatten(),\\n    layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\\n    layers.Dropout(0.5),\\n    layers.Dense(1, activation='sigmoid')\\n])\\n\\noptimizer = Adam(learning_rate=0.001)\\nsecondModel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\\nsecondModel.summary()\\nsecondModel.save('models/secondModel.h5')\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''secondModel = tf.keras.Sequential([\n",
    "    layers.Embedding(500, 16, input_length=40),\n",
    "    layers.Conv1D(128, 5, activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=4),\n",
    "    layers.Conv1D(64, 5, activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=4),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "secondModel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "secondModel.summary()\n",
    "secondModel.save('models/secondModel.h5')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#earlyStopping = EarlyStopping(min_delta=0.001, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'history = secondModel.fit(\\n    training_padded,\\n    trainingTargetsArray,\\n    epochs=15,\\n    validation_data=(testing_padded, testingTargetsArray),\\n    callbacks=[earlyStopping]\\n)'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''history = secondModel.fit(\n",
    "    training_padded,\n",
    "    trainingTargetsArray,\n",
    "    epochs=15,\n",
    "    validation_data=(testing_padded, testingTargetsArray),\n",
    "    callbacks=[earlyStopping]\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loss, accuracy = secondModel.evaluate(testing_padded, testingTargetsArray)\\nprint(f\"Test Loss: {loss:.4f}\")\\nprint(f\"Test Accuracy: {accuracy:.4f}\")'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''loss, accuracy = secondModel.evaluate(testing_padded, testingTargetsArray)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Visualizaci√≥n de las m√©tricas\\nplt.figure(figsize=(12, 6))\\nplt.subplot(1, 2, 1) # Loss\\nplt.plot(history.history['loss'], label='Training Loss')\\nplt.plot(history.history['val_loss'], label='Validation Loss')\\nplt.title('Training and Validation Loss')\\nplt.xlabel('Epochs')\\nplt.ylabel('Loss')\\nplt.legend()\\nplt.subplot(1, 2, 2) # Accuracy\\nplt.plot(history.history['accuracy'], label='Training Accuracy')\\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\\nplt.title('Training and Validation Accuracy')\\nplt.xlabel('Epochs')\\nplt.ylabel('Accuracy')\\nplt.legend()\\nplt.show()\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Visualizaci√≥n de las m√©tricas\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1) # Loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2) # Accuracy\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üü¢ SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"vectorizer = TfidfVectorizer(max_features=500, stop_words='english')\\nX = vectorizer.fit_transform(sentences)\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, targets, test_size=0.2, random_state=42)\\n\\nsvm_model = SVC(kernel='linear', C=1.0, random_state=42)\\nsvm_model.fit(X_train, y_train)\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''vectorizer = TfidfVectorizer(max_features=500, stop_words='english')\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, targets, test_size=0.2, random_state=42)\n",
    "\n",
    "svm_model = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "svm_model.fit(X_train, y_train)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Predicci√≥n y evaluaci√≥n del modelo\\ny_pred = svm_model.predict(X_test)\\naccuracy = accuracy_score(y_test, y_pred)\\nprint(f\"Test Accuracy: {accuracy:.4f}\")\\n\\n# Reporte de clasificaci√≥n\\nprint(\"\\nClassification Report:\")\\nprint(classification_report(y_test, y_pred))'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Predicci√≥n y evaluaci√≥n del modelo\n",
    "y_pred = svm_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Reporte de clasificaci√≥n\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inciso 7. ‚úÖ Clasificaci√≥n de tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'first_10_tweets = dataTest[\\'text\\'].head(6).tolist()\\nfirst_10_sequences = tokenizer.texts_to_sequences(first_10_tweets)\\nfirst_10_padded = pad_sequences(first_10_sequences, maxlen=40, padding=\\'post\\', truncating=\\'post\\')\\npredictions = firstModel.predict(first_10_padded)\\npredictions2 = secondModel.predict(first_10_padded)\\npredictions3 = svm_model.predict(vectorizer.transform(first_10_tweets))\\n\\nprint(\"üîÆ Predicciones del primer modelo:\\n\")\\nfor i, prediction in enumerate(predictions):\\n    print(f\"Tweet: {first_10_tweets[i]}\")\\n    if prediction[0] > 0.5:\\n        print(\"Predicci√≥n: Este tweet se refiere a un desastre.\")\\n    else:\\n        print(\"Predicci√≥n: Este tweet NO se refiere a un desastre.\")\\n    print(\"\\n\")\\n\\nprint(\"üîÆ Predicciones del segundo modelo:\\n\")\\nfor i, prediction in enumerate(predictions2):\\n    print(f\"Tweet: {first_10_tweets[i]}\")\\n    if prediction[0] > 0.5:\\n        print(\"Predicci√≥n: Este tweet se refiere a un desastre.\")\\n    else:\\n        print(\"Predicci√≥n: Este tweet NO se refiere a un desastre.\")\\n    print(\"\\n\")\\n\\nprint(\"üîÆ Predicciones del modelo SVM:\\n\")\\nfor i, prediction in enumerate(predictions3):\\n    print(f\"Tweet: {first_10_tweets[i]}\")\\n    if prediction == 1:\\n        print(\"Predicci√≥n: Este tweet se refiere a un desastre.\")\\n    else:\\n        print(\"Predicci√≥n: Este tweet NO se refiere a un desastre.\")\\n    print(\"\\n\")'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''first_10_tweets = dataTest['text'].head(6).tolist()\n",
    "first_10_sequences = tokenizer.texts_to_sequences(first_10_tweets)\n",
    "first_10_padded = pad_sequences(first_10_sequences, maxlen=40, padding='post', truncating='post')\n",
    "predictions = firstModel.predict(first_10_padded)\n",
    "predictions2 = secondModel.predict(first_10_padded)\n",
    "predictions3 = svm_model.predict(vectorizer.transform(first_10_tweets))\n",
    "\n",
    "print(\"üîÆ Predicciones del primer modelo:\\n\")\n",
    "for i, prediction in enumerate(predictions):\n",
    "    print(f\"Tweet: {first_10_tweets[i]}\")\n",
    "    if prediction[0] > 0.5:\n",
    "        print(\"Predicci√≥n: Este tweet se refiere a un desastre.\")\n",
    "    else:\n",
    "        print(\"Predicci√≥n: Este tweet NO se refiere a un desastre.\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(\"üîÆ Predicciones del segundo modelo:\\n\")\n",
    "for i, prediction in enumerate(predictions2):\n",
    "    print(f\"Tweet: {first_10_tweets[i]}\")\n",
    "    if prediction[0] > 0.5:\n",
    "        print(\"Predicci√≥n: Este tweet se refiere a un desastre.\")\n",
    "    else:\n",
    "        print(\"Predicci√≥n: Este tweet NO se refiere a un desastre.\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(\"üîÆ Predicciones del modelo SVM:\\n\")\n",
    "for i, prediction in enumerate(predictions3):\n",
    "    print(f\"Tweet: {first_10_tweets[i]}\")\n",
    "    if prediction == 1:\n",
    "        print(\"Predicci√≥n: Este tweet se refiere a un desastre.\")\n",
    "    else:\n",
    "        print(\"Predicci√≥n: Este tweet NO se refiere a un desastre.\")\n",
    "    print(\"\\n\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"training_sequences = tokenizer.texts_to_sequences(trainingSentences)\\ntraining_padded = pad_sequences(training_sequences, maxlen=40, padding='post', truncating='post')\\npredictions_train = firstModel.predict(training_padded)\\npredictions_train = (predictions_train > 0.5).astype(int) \\ntrue_labels_train = np.array(trainingTargets) \\ncm_train = confusion_matrix(true_labels_train, predictions_train)\\nplt.figure(figsize=(8, 6))\\nsns.heatmap(cm_train, annot=True, fmt='d', cmap='Blues', xticklabels=['No Desastre', 'Desastre'], yticklabels=['No Desastre', 'Desastre'])\\nplt.xlabel('Predicci√≥n')\\nplt.ylabel('Realidad')\\nplt.title('Diagrama de Confusi√≥n - Conjunto de Entrenamiento Primer Modelo')\\nplt.show()\\n\\ntraining_sequences = tokenizer.texts_to_sequences(trainingSentences)\\ntraining_padded = pad_sequences(training_sequences, maxlen=40, padding='post', truncating='post')\\npredictions_train = secondModel.predict(training_padded)\\npredictions_train = (predictions_train > 0.5).astype(int) \\ntrue_labels_train = np.array(trainingTargets) \\ncm_train = confusion_matrix(true_labels_train, predictions_train)\\nplt.figure(figsize=(8, 6))\\nsns.heatmap(cm_train, annot=True, fmt='d', cmap='Blues', xticklabels=['No Desastre', 'Desastre'], yticklabels=['No Desastre', 'Desastre'])\\nplt.xlabel('Predicci√≥n')\\nplt.ylabel('Realidad')\\nplt.title('Diagrama de Confusi√≥n - Conjunto de Entrenamiento Segundo Modelo')\\nplt.show()\\n\\ny_pred = svm_model.predict(X_train)\\ncm_train_svm = confusion_matrix(y_train, y_pred)\\n\\n# Graficamos la matriz de confusi√≥n\\nplt.figure(figsize=(8, 6))\\nsns.heatmap(cm_train_svm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Desastre', 'Desastre'], yticklabels=['No Desastre', 'Desastre'])\\nplt.xlabel('Predicci√≥n')\\nplt.ylabel('Realidad')\\nplt.title('Diagrama de Confusi√≥n - Conjunto de Entrenamiento SVM')\\nplt.show()\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''training_sequences = tokenizer.texts_to_sequences(trainingSentences)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=40, padding='post', truncating='post')\n",
    "predictions_train = firstModel.predict(training_padded)\n",
    "predictions_train = (predictions_train > 0.5).astype(int) \n",
    "true_labels_train = np.array(trainingTargets) \n",
    "cm_train = confusion_matrix(true_labels_train, predictions_train)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_train, annot=True, fmt='d', cmap='Blues', xticklabels=['No Desastre', 'Desastre'], yticklabels=['No Desastre', 'Desastre'])\n",
    "plt.xlabel('Predicci√≥n')\n",
    "plt.ylabel('Realidad')\n",
    "plt.title('Diagrama de Confusi√≥n - Conjunto de Entrenamiento Primer Modelo')\n",
    "plt.show()\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(trainingSentences)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=40, padding='post', truncating='post')\n",
    "predictions_train = secondModel.predict(training_padded)\n",
    "predictions_train = (predictions_train > 0.5).astype(int) \n",
    "true_labels_train = np.array(trainingTargets) \n",
    "cm_train = confusion_matrix(true_labels_train, predictions_train)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_train, annot=True, fmt='d', cmap='Blues', xticklabels=['No Desastre', 'Desastre'], yticklabels=['No Desastre', 'Desastre'])\n",
    "plt.xlabel('Predicci√≥n')\n",
    "plt.ylabel('Realidad')\n",
    "plt.title('Diagrama de Confusi√≥n - Conjunto de Entrenamiento Segundo Modelo')\n",
    "plt.show()\n",
    "\n",
    "y_pred = svm_model.predict(X_train)\n",
    "cm_train_svm = confusion_matrix(y_train, y_pred)\n",
    "\n",
    "# Graficamos la matriz de confusi√≥n\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_train_svm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Desastre', 'Desastre'], yticklabels=['No Desastre', 'Desastre'])\n",
    "plt.xlabel('Predicci√≥n')\n",
    "plt.ylabel('Realidad')\n",
    "plt.title('Diagrama de Confusi√≥n - Conjunto de Entrenamiento SVM')\n",
    "plt.show()'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
